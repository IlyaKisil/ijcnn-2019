{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Application of Tensor Decompositions\n",
    "[Return to Table of Contents](./0_Table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Utils for this tutorial\n",
    "from ijcnn import print_basic_metrics\n",
    "from ijcnn.io import ETH80, plot_tensors\n",
    "\n",
    "#\n",
    "from hottbox.version import __version__ as hottbox_version\n",
    "print(\"HOTTBOX version: {}\".format(hottbox_version))\n",
    "\n",
    "# Initialise dataset\n",
    "eth = ETH80()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Tucker representation\n",
    "\n",
    "<img src=\"./imgs/TensorTKD.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Any tensor of arbitrarily large order can be decomposed in the Tucker form. As illustrated above, a tensor $\\mathbf{\\underline{X}} \\in \\mathbb{R}^{I \\times J \\times K}$ can be represented as a dense core tensor $\\mathbf{\\underline{G}}$ and a set of factor matrices $\\mathbf{A} \\in \\mathbb{R}^{I \\times Q}, \\mathbf{B} \\in \\mathbb{R}^{J \\times R}$ and $\\mathbf{C} \\in\n",
    "\\mathbb{R}^{K \\times P}$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\underline{X}} = \\mathbf{\\underline{G}} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C} = \\Big[    \\mathbf{\\underline{G}} ;  \\mathbf{A},  \\mathbf{B}, \\mathbf{C}      \\Big]\n",
    "$$\n",
    "\n",
    "Essentially, this means that the original data, $\\mathbf{\\underline{X}}$, is compressed into a dense core tensor, $\\mathbf{\\underline{G}}$, of much smaller size. The inverse of this operation is projection of a core tensor onto the subspaces spanned by the corresponding factor matrices, $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$.\n",
    "\n",
    "On practice, there exist several computational methods to represent N-dimensional array in Tucker form. Such algorithms are combined into one framework, namely the Tucker Decomposition. Here we will consider one of the most commonly used implementation: Higher Order Singular Value Decomposition ([HOSVD](#Higher-Order-Singular-Value-Decomposition-(HOSVD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Higher Order Singular Value Decomposition (HOSVD)\n",
    "\n",
    "The HOSVD is a special case of the Tucker decomposition, in which all the factor matrices are constrained to be orthogonal. They are computed as truncated version of the left singular matrices of all possible mode-$n$ unfoldings of tensor $\\mathbf{\\underline{X}}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{X}_{(1)} &= \\mathbf{U}_1  \\mathbf{\\Sigma}_1 \\mathbf{V}_1^T \\quad \\rightarrow \\quad \\mathbf{A} = \\mathbf{U}_1[1:R_1]\\\\\n",
    "\\mathbf{X}_{(2)} &= \\mathbf{U}_2  \\mathbf{\\Sigma}_2 \\mathbf{V}_2^T \\quad \\rightarrow \\quad \\mathbf{B} = \\mathbf{U}_2[1:R_2] \\\\\n",
    "\\mathbf{X}_{(3)} &= \\mathbf{U}_3  \\mathbf{\\Sigma}_3 \\mathbf{V}_3^T \\quad \\rightarrow \\quad \\mathbf{C} = \\mathbf{U}_3[1:R_3] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After factor matrices are obtained, the core tensor $\\mathbf{\\underline{G}}$ is computed as\n",
    "\n",
    "$$\n",
    "\\mathbf{\\underline{G}} = \\mathbf{\\underline{X}} \\times_1 \\mathbf{A}^T \\times_2 \\mathbf{B}^T \\times_3 \\mathbf{C}^T        \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Multi-linear rank\n",
    "\n",
    "The **multi-linear rank** of a tensor $\\mathbf{\\underline{X}} \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}$ is the $N$-tuple $(R_1, \\dots, R_N)$ where each $R_n$ is the rank of the subspace spanned by mode-$n$ fibers, i.e. $R_n = \\text{rank} \\big( \\mathbf{X}_{(n)} \\big)$. Thus, for our order-$3$ tensor the multi-linear rank is $(R_1, R_2, R_3)$. Multi-linear rank provides flexibility in compression and approximation of the original tensor.\n",
    "\n",
    "> **NOTE:** For a tensor of order $N$ the values $R_1, R_2, \\dots , R_N$ are not necessarily the same, whereas, for matrices (tensors of order 2) the equality $R_1 = R_2$ always holds, where $R_1$ and $R_2$ are the matrix column rank and row rank respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Image compression \n",
    "\n",
    "Color images can be naturally represented as a tensor of order three with the shape `(height x width x channels)` where channels are, for example, Red, Blue and Green (RGB)\n",
    "\n",
    "<img src=\"./imgs/image_to_base_colors.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "By keeping its original structure, allows to apply methods from multi-linear analysis. For instance, we can employ algorithms for Tucker decompositions in order to commress oringinal informaiton by varying values of desired multi-linear rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hottbox.algorithms.decomposition import HOSVD\n",
    "from hottbox.utils.generation import residual_tensor\n",
    "\n",
    "data, _ = eth.get_samples(objects=[\"car\"],\n",
    "                          angle_1=[\"045\"],\n",
    "                          angle_2=[\"000\"])\n",
    "\n",
    "# Select only one sample, try out different samples, e.g. [3]\n",
    "tensor = data[0]\n",
    "\n",
    "# Set desired multi-linear rank, try out different values, e.g. (25, 25, 1)\n",
    "ml_rank = (25, 25, 2)\n",
    "\n",
    "# Initialise decomposition algorithm\n",
    "algorithm = HOSVD()\n",
    "\n",
    "# Perform compression via HOSVD\n",
    "tensor_tkd = algorithm.decompose(tensor, rank=ml_rank, keep_meta=True)\n",
    "\n",
    "# Perform reconstruction\n",
    "tensor_reconstructed = tensor_tkd.reconstruct()\n",
    "\n",
    "# Obtaion residual part\n",
    "tensor_residual = residual_tensor(tensor, tensor_tkd)\n",
    "\n",
    "# Preview results\n",
    "print_basic_metrics(tensor, tensor_tkd)\n",
    "plot_tensors([tensor, tensor_reconstructed, tensor_residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appllication: Tensor Ensemble Learning\n",
    "\n",
    "<img src=\"./imgs/wisdom_of_the_crowd.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The phenomenon of the wisdom of the crowd has been known for a very long time. The machine learning community have adopted this concept under the framework of **ensemble learning**. Key points:\n",
    "- Every predictor should perform better than a random guess\n",
    "- The errors of individual predictors should be uncorrelated\n",
    "- Heterogeneous set of base learning algorithms - diversity of the individual hypotheses\n",
    "- Homogeneous set of base classifiers and exposing each member to a subset of the training data\n",
    "\n",
    "<div style=\"font-size:1.7em;\">Ensemble Learning</div>  | | | <div style=\"font-size:1.7em;\">Tensor Ensemble Learning</div>\n",
    ":--- | --- | ---:\n",
    "<img src=\"./imgs/EL_model_generation.png\" alt=\"Drawing\" style=\"width: 500px;\"/> | $\\Longleftrightarrow$ | <img src=\"./imgs/TEL_training_stage.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <span style=\"color:blue; font-family:Georgia; font-size:1.7em;\">\n",
    "        Ensemble Learning + Tensor Decompositions = Tensor Ensemble Learning\n",
    "    </span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/TEL_general_concept.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Implementation notes:\n",
    "- For convenience, Stage 1 is performed on the whole batch of samples, but for each sample separately. Can be implemented in an online manner.\n",
    "- Stage 2 and 3 are connected, therefore for simplification it is hidden from the end user  (so as Stage 4). Instead, we provide a top level API (similar to [**scikit-learn**](https://scikit-learn.org/stable/index.html)).\n",
    "- Base classifiers could be anything, but there are two main requirements: \n",
    "    1. Their number should correspond to the decomposition (multi-linear rank) employed at Stage 1.\n",
    "    1. They should have **scikit-liearn** type API, that is:\n",
    "    ```python\n",
    "    base_clf.fit(X, y)\n",
    "    base_clf.predict(X)\n",
    "    base_clf.predict_proba(X)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from hottbox.algorithms.decomposition import HOSVD\n",
    "from hottbox.algorithms.classification import TelVI\n",
    "\n",
    "# Get original multi-dimensional samples\n",
    "# data, labels = eth.get_samples(objects=[\"dog\", \"car\"], angle_1=[\"000\"], angle_2=[\"000\"])\n",
    "# data, labels = eth.get_samples([\"dog\", \"car\"], [\"045\"], [])\n",
    "data, labels = eth.get_samples([\"dog\", \"car\"], [], [])\n",
    "\n",
    "# Represent each sample in Tucker form and store it in a list\n",
    "algorithm = HOSVD()\n",
    "ml_rank = (5, 5, 2)\n",
    "data_transformed = [algorithm.decompose(sample, rank=(5, 5, 2)) for sample in data]\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_transformed, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialise classifier\n",
    "base_clf_required = np.sum(ml_rank)\n",
    "telvi = TelVI(base_clf=[SVC(gamma='auto') for _ in range(base_clf_required)],\n",
    "              probability=True,\n",
    "              verbose=True)\n",
    "\n",
    "# Train classifer\n",
    "telvi.fit(X_train, y_train)\n",
    "score = telvi.score(X_test, y_test)\n",
    "print(\"Classification accuracy: {:.2f}%\\n\".format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters of the base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "search_params = [dict(gamma=[0.001, 0.01, 1, 10], C=[0.1, 1, 10, 100]) for _ in range(12)]\n",
    "\n",
    "print(\"\\n\\tPerforming grid search for each base classifer\")\n",
    "telvi.grid_search(X_train, y_train, search_params)\n",
    "\n",
    "print(\"\\n\\tTrain base classifiers with optimal hyperparameters\")\n",
    "telvi.fit(X_train, y_train)\n",
    "\n",
    "score = telvi.score(X_test, y_test)\n",
    "print(\"\\nClassification accuracy: {:.2f}%\\n\".format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of original and transformed samples\n",
    "plot_tensors([X_train[0].reconstruct(),\n",
    "              X_train[1].reconstruct(),\n",
    "              X_test[0].reconstruct(),\n",
    "              X_test[1].reconstruct()\n",
    "             ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for interacting with ETH-80 dataset\n",
    "\n",
    "[**ETH-80 dataset**](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/analyzing-appearance-and-contour-based-methods-for-object-categorization/) consists of 3280 colour images (128×128 pixels) from 8 categories: *apple, car, cow, cup, dog, horse, pear, tomato*. Each category contains 10 different objects with 41 views per object, spaced equally over the\n",
    "viewing hemisphere. For ease of interaction with this dataset we provide you with `ETH80` class and some other utility functions (source code could be found [here](../ijcnn/ijcnn/io.py)).\n",
    "\n",
    "Interaction with the dataset include two main steps.\n",
    "- Initialisation of the dataset (can be done only once)\n",
    "- Obtaining samples of interest and corresponding labels\n",
    "\n",
    "```python\n",
    "# Initialise dataset\n",
    "eth = ETH80()\n",
    "\n",
    "# By default returns all available samples\n",
    "# as a list of 'Tensor' objects and array of labels\n",
    "data, labels = eth.get_samples()\n",
    "```\n",
    "\n",
    "In order to work with only a portion of the dataset, you can pass parameters to `get_samples()`. To see all available options use the following properties of the `ETH80` class\n",
    "\n",
    "```python\n",
    "# List of object names\n",
    "eth.available_objects\n",
    "\n",
    "# List of angle pairs\n",
    "eth.available_angle_pairs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only a portion of the whole dataset\n",
    "data, labels = eth.get_samples(objects=[\"car\", \"dog\"], \n",
    "                               angle_1=[\"000\", \"045\"], \n",
    "                               angle_2=[\"000\"])\n",
    "print(\"Number of samples: {}\\n\".format(len(data)))\n",
    "plot_tensors(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can inspect meta data this data set with the use of `meta_data` property of `ETH80` class, which returns a [pandas dataframe](https://www.geeksforgeeks.org/python-pandas-dataframe/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth.meta_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcnn-2019",
   "language": "python",
   "name": "ijcnn-2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
